{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Choosing a DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First going to train on Wikipedia Data.\n",
    "\n",
    "And to avoid additional Computation & Internet Costs, would be going with using API to stream the data instead of downloading all at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries versions:\n",
      "\n",
      "requests\t2.32.3\n",
      "json\t\t2.0.9\n"
     ]
    }
   ],
   "source": [
    "print(\"Libraries versions:\\n\")\n",
    "print(f\"requests\\t{requests.__version__}\")\n",
    "print(f\"json\\t\\t{json.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROGRESS_FILE = 'progress.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r') as file:\n",
    "            return json.load(file)\n",
    "    return {\"last_title\": None, \"fetched_titles\": set()}\n",
    "\n",
    "def save_progress(progress):\n",
    "    with open(PROGRESS_FILE, 'w') as file:\n",
    "        json.dump(progress, file)\n",
    "\n",
    "def fetch_wikipedia_articles(limit=100, batch_size=10, output_dir='data_chunks'):\n",
    "    '''Fetches a list of Wikipedia articles and their content, starting from the last fetched title.'''\n",
    "    progress = load_progress()\n",
    "    last_title = progress[\"last_title\"]\n",
    "    fetched_titles = progress[\"fetched_titles\"]\n",
    "\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"allpages\",\n",
    "        \"aplimit\": batch_size,\n",
    "        \"apfilterredir\": \"nonredirects\",\n",
    "    }\n",
    "\n",
    "    if last_title:\n",
    "        params[\"apfrom\"] = last_title\n",
    "\n",
    "    articles = []\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    while len(articles) < limit:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "        pages = data[\"query\"][\"allpages\"]\n",
    "\n",
    "        for page in pages:\n",
    "            page_title = page[\"title\"]\n",
    "            if page_title not in fetched_titles:\n",
    "                page_content = fetch_page_content(page_title)\n",
    "                articles.append(page_content)\n",
    "                fetched_titles.add(page_title)\n",
    "\n",
    "                # Save the article content to a file\n",
    "                save_article_to_disk(page_title, page_content, output_dir)\n",
    "\n",
    "                if len(articles) >= limit:\n",
    "                    break\n",
    "\n",
    "        if \"continue\" not in data:\n",
    "            break\n",
    "\n",
    "        params.update(data[\"continue\"])\n",
    "\n",
    "    # Update progress\n",
    "    progress[\"last_title\"] = pages[-1][\"title\"] if pages else last_title\n",
    "    progress[\"fetched_titles\"] = list(fetched_titles)\n",
    "    save_progress(progress)\n",
    "\n",
    "    return articles\n",
    "\n",
    "def fetch_page_content(page_title):\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": page_title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    page_id = next(iter(pages))\n",
    "    page_content = pages[page_id][\"extract\"]\n",
    "\n",
    "    return page_content\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove invalid characters from the filename\n",
    "    return re.sub(r'[\\\\/:*?\"<>|]', '', filename)\n",
    "\n",
    "def save_article_to_disk(page_title, page_content, output_dir):\n",
    "    file_path = os.path.join(output_dir, f\"{sanitize_filename(page_title)}.txt\")\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(page_content)\n",
    "\n",
    "def fetch_random_wikipedia_articles(limit=100, output_dir='data_chunks'):\n",
    "    '''Fetches a list of random Wikipedia articles and their content.'''\n",
    "    progress = load_progress()\n",
    "    fetched_titles = set(progress[\"fetched_titles\"])\n",
    "\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"random\",\n",
    "        \"rnlimit\": limit,\n",
    "        \"rnnamespace\": 0,  # Main namespace\n",
    "    }\n",
    "\n",
    "    articles = []\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data[\"query\"][\"random\"]\n",
    "\n",
    "    for page in pages:\n",
    "        page_title = page[\"title\"]\n",
    "        if page_title not in fetched_titles:\n",
    "            page_content = fetch_page_content(page_title)\n",
    "            articles.append(page_content)\n",
    "            fetched_titles.add(page_title)\n",
    "\n",
    "            # Save the article content to a file\n",
    "            save_article_to_disk(page_title, page_content, output_dir)\n",
    "\n",
    "    # Update progress\n",
    "    progress[\"fetched_titles\"] = list(fetched_titles)\n",
    "    save_progress(progress)\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch articles starting from the last fetched title\n",
    "# articles = fetch_wikipedia_articles(limit=10, batch_size=3)\n",
    "\n",
    "# Fetch random articles\n",
    "random_articles = fetch_random_wikipedia_articles(limit=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "with open(\"progress.json\", \"r\") as file:\n",
    "    file = json.load(file)\n",
    "    print(len(file['fetched_titles']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2 Training the Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
